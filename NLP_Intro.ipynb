{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Areas covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing covers a wide range of different activities. This notebook covers the basic steps of preparing data for NLP applications:\n",
    "1. Word tokenization\n",
    "2. Creating word embeddings\n",
    "3. POS tagging\n",
    "4. Lemmatization\n",
    "5. Tree parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.chunk import RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization is the task of separating text into words.\n",
    "\n",
    "The most trivial way of tokenizing text is to have Python split the text string based on spaces. For English and other European languages, this does not always do what we want—punctuation will typically be mishandled, resulting in chunks that are not words. Moreover, many Asian languages do not place spaces between words, meaning that this strategy won't work at all for those words.\n",
    "\n",
    "The method `word_tokenize` from `nltk` is useful in that it can accurately split text from languages that use word-based spacing. Other languages that do not do this need special modules to achieve this, or, in the case of languages like Hmong that place spaces between syllables, where morpheme boundaries coincide with syllable boundaries, `word_tokenize` can be used with IOB tagging (explained below) at the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'method', 'word_tokenize', 'from', 'nltk', 'is', 'useful', 'in', 'that', 'it', 'can', 'accurately', 'split', 'text', 'from', 'languages', 'that', 'use', 'word-based', 'spacing', '.', 'Other', 'languages', 'that', 'do', 'not', 'do', 'this', 'need', 'special', 'modules', 'to', 'achieve', 'this', ',', 'or', ',', 'in', 'the', 'case', 'of', 'languages', 'like', 'Hmong', 'that', 'place', 'spaces', 'between', 'syllables', ',', 'where', 'morpheme', 'boundaries', 'coincide', 'with', 'syllable', 'boundaries', ',', 'word_tokenize', 'can', 'be', 'used', 'with', 'IOB', 'tagging', '(', 'explained', 'below', ')', 'at', 'the', 'word', 'level', '.']\n"
     ]
    }
   ],
   "source": [
    "text = 'The method word_tokenize from nltk is useful in that it can accurately split text from languages that use word-based spacing. Other languages that do not do this need special modules to achieve this, or, in the case of languages like Hmong that place spaces between syllables, where morpheme boundaries coincide with syllable boundaries, word_tokenize can be used with IOB tagging (explained below) at the word level.'\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are abstract representations of words in the form of lists of numbers acting as vectors, which computers can use to train models based on the words found in the embeddings.\n",
    "\n",
    "A simple version might have _cat_ as [ 0.36403364 -0.49103507  0.18100677  0.82723975  0.641248    0.34181604\n",
    "  0.01125114 -0.7229596   0.24586748 -0.5252472 ].\n",
    "\n",
    "We create a very simple, illustrative word embedding system for English below, using the Brown corpus found in `nltk.corpus.brown`, and the `Word2Vec` module from `gensim.models`. Here, sentences are drawn from the Brown corpus using `brown.sents()`, as Word2Vec needs sentences as input. \n",
    "\n",
    "The `Word2Vec` initialization method includes as parameters:\n",
    "1. the data (`sentences`)\n",
    "2. `iter`: the number of times to pass through the data while training\n",
    "3. `workers`: the number of processes to use while training\n",
    "4. `size`: the magnitude of the word embedding vector\n",
    "5. `window`: the number of words on either side of the current word to consider\n",
    "6. `min_count`: the number of times a word should occur before it receives its own word embedding vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = brown.sents()\n",
    "model = Word2Vec(sentences, iter=10, workers=10, size=10, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see what the word embedding vectors look like for individual words using `model.wv['word']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.36403364 -0.49103507  0.18100677  0.82723975  0.641248    0.34181604\n",
      "  0.01125114 -0.7229596   0.24586748 -0.5252472 ]\n",
      "[-0.51304495  0.0516611  -1.1650859   0.30826917 -0.3926486  -1.0635322\n",
      " -3.607118    0.06678296  0.6079878  -1.8219658 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['cat'])\n",
    "print(model.wv['new'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting trick is to use word embeddings to find similar words, using `model.wv.most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('black', 0.968340277671814), ('brown', 0.9671033620834351), ('green', 0.9663865566253662), ('gray', 0.9647021293640137), ('red', 0.9555657505989075), ('white', 0.9543337821960449), ('thin', 0.9507400989532471), ('pink', 0.9382452964782715), ('deep', 0.9381362199783325), ('pale', 0.9359599351882935)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(['blue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try a color word, lots of other color words also appear: _black_, _brown_, _green_, _gray_, _red_, _white_, and _pink_, along with other adjectives of physical description such as _thin_, and especially those associated with colors, such as _deep_ (as in _deep red_) or _pale_ (as in _pale red_).\n",
    "\n",
    "However, given the small size of the vectors (only 10!) and the small number of iterations (also only 10!), a lot of noise will be seen for other words—_lawyer_ will get results like _governor_ and _critic_ along with all sorts of strange things, such as _Holy_, _old_, and _thinks_. This can be avoided with larger vectors and a greater number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, however, are more robust word embedding training methods. Recent methods include BERT, XLNet, and ALBERT. Note that these other methods require computing power far greater than what is available on a typical laptop or desktop. On one occasion, out of curiosity I attempted to run ALBERT on my laptop, and based on the rate it was processing the data, training the model for Hmong based on the SCH Corpus would have taken about three years. The good news is these more robust models have been trained for many languages by the Google team already, but only those with a Wikipedia version with high volumes of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging assigns word class labels to each word in a text. This can not only make the text more accessible for corpus-based data mining, but it also typically serves as a basis for more advanced NLP applications.\n",
    "`nltk` has the method `pos_tag` that can do this automatically for English, because `nltk` already has a tagged corpus available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('method', 'NN'), ('word_tokenize', 'NN'), ('from', 'IN'), ('nltk', 'NN'), ('is', 'VBZ'), ('useful', 'JJ'), ('in', 'IN'), ('that', 'IN'), ('it', 'PRP'), ('can', 'MD'), ('accurately', 'RB'), ('split', 'VB'), ('text', 'NN'), ('from', 'IN'), ('languages', 'NNS'), ('that', 'WDT'), ('use', 'VBP'), ('word-based', 'JJ'), ('spacing', 'NN'), ('.', '.'), ('Other', 'JJ'), ('languages', 'NNS'), ('that', 'WDT'), ('do', 'VBP'), ('not', 'RB'), ('do', 'VB'), ('this', 'DT'), ('need', 'NN'), ('special', 'JJ'), ('modules', 'NNS'), ('to', 'TO'), ('achieve', 'VB'), ('this', 'DT'), (',', ','), ('or', 'CC'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('case', 'NN'), ('of', 'IN'), ('languages', 'NNS'), ('like', 'IN'), ('Hmong', 'NNP'), ('that', 'WDT'), ('place', 'NN'), ('spaces', 'NNS'), ('between', 'IN'), ('syllables', 'NNS'), (',', ','), ('where', 'WRB'), ('morpheme', 'NN'), ('boundaries', 'NNS'), ('coincide', 'VBP'), ('with', 'IN'), ('syllable', 'JJ'), ('boundaries', 'NNS'), (',', ','), ('word_tokenize', 'VB'), ('can', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('with', 'IN'), ('IOB', 'NNP'), ('tagging', 'VBG'), ('(', '('), ('explained', 'VBN'), ('below', 'IN'), (')', ')'), ('at', 'IN'), ('the', 'DT'), ('word', 'NN'), ('level', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_tagged_tokens = pos_tag(tokens)\n",
    "print(pos_tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, POS tags typically have 2-3 letters representing the word class. _DT_ is 'determiner', _NN_ is 'noun', _PRP_ is 'personal pronoun', and so on.\n",
    "\n",
    "For other languages, POS-tagging models have to be trained from scratch, using the word embeddings described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of determining the lemma (~ word root, base form, and so on) of each word. In other words, _walking_ produces _walk_, _bigger_ produces _big_, and so on. This is important as it makes recognizing words with the same root simpler. For linguistics research, this can be especially useful for finding all of the forms of a given root and finding their distribution.\n",
    "To do lemmatization, we can use `WordNetLemmatizer` from `nltk.stem`. `WordNetLemmatizer` is based on WordNet, a database that represents words and the relations between them for English.\n",
    "For other languages, a specific lemmatizer has to be developed. Several major world languages already have one, while Hmong, which has less than 30 affixes and word roots that are readily obvious, would not benefit from lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('saw', 'VBD'), ('them', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('shop', 'NN'), ('everyday', 'NN'), ('when', 'WRB'), ('I', 'PRP'), ('was', 'VBD'), ('younger', 'JJR'), ('.', '.')]\n",
      "['I', 'saw', 'them', 'at', 'the', 'shop', 'everyday', 'when', 'I', 'be', 'young', '.']\n"
     ]
    }
   ],
   "source": [
    "#from nltk import download\n",
    "#download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sent = 'I saw them at the shop everyday when I was younger.'\n",
    "tokens = word_tokenize(sent)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "print(tagged_tokens)\n",
    "tag_conversions = {'VBD':'v', 'JJR':'a'}\n",
    "\n",
    "# loop through tokens to produce result\n",
    "lemmas = []\n",
    "for token in tagged_tokens:\n",
    "    if token[1] in tag_conversions.keys():\n",
    "        lemma = lemmatizer.lemmatize(token[0], tag_conversions[token[1]])\n",
    "    else:\n",
    "        lemma = lemmatizer.lemmatize(token[0])\n",
    "    lemmas.append(lemma)\n",
    "    \n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree parsing is combining words into constituent phrases, such as adjective + noun or demonstrative + noun to make a noun phrase.\n",
    "\n",
    "This can be done with `nltk` through the use of manual rules written as strings, which are then loaded into a `RegexpParser` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP kuv/PN) yog/VV (NP ib/QU tug/CL neeg/NN))\n"
     ]
    }
   ],
   "source": [
    "phrase = 'kuv/PN yog/VV ib/QU tug/CL neeg/NN'\n",
    "tokens = phrase.split(' ')\n",
    "tagged_tokens = [tuple(w.split('/')) for w in tokens]\n",
    "\n",
    "NP_rule = \"\"\"NP: \n",
    "{<QU><CL><NN>}\n",
    "{<PN>}\"\"\"\n",
    "\n",
    "parser = RegexpParser(NP_rule)\n",
    "result = parser.parse(tagged_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond these basics, many other things can be done with NLP, based on the steps discussed above:\n",
    "1. Named entity recognition (recognizing names of people, companies, places, etc.) and linking entities in relationships\n",
    "2. Sentiment classification (is an evaluation good, bad, etc.)\n",
    "3. Text classification (spam vs. non-spam, genre, etc.)\n",
    "4. Machine translation\n",
    "5. Spell checker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
